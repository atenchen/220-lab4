{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atenchen/220-lab4/blob/master/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Author Information\n",
        "Name: \\<Alvin Chen\\>\n",
        "\n",
        "B-Number: \\<B00760047\\>\n",
        "\n",
        "Email: \\<achen172@binghamton.edu\\>"
      ],
      "metadata": {
        "id": "tUheapJCOaxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Instructions \n",
        "### Due October 7th, 11:59 PM.\n",
        "\n",
        "In the following assignment, you will be implementing functions and their analytical derivatives to train linear classifiers and neural networks on the MNIST dataset. \n",
        "\n",
        "Functions and cells that need to be implemented are marked with a bold **implement** keyword or clearly marked in the experiments section. \n",
        "\n",
        "The experiments section for each classifier also need to be implemented. You should follow the instructions above the cell. You may also add additional cells. \n",
        "\n",
        "Cells marked **run** need to be run to set up the appropriate infrastructure, but do not need to be modified. Make sure you have run the previous cells before running the current cell, or you may get an error.\n",
        "\n",
        "Submission will be via GitHub Classroom. **You are required to have at least 10 commits for this assignment.**\n"
      ],
      "metadata": {
        "id": "jqLULe5pxwpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import statements\n",
        "\n",
        "**Run** the cell to import the packages needed for the code below. You may other packages but ask first. "
      ],
      "metadata": {
        "id": "Hnw_h7A1Orc9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JLEavoS9O9g-"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Backpropagation\n"
      ],
      "metadata": {
        "id": "VIBNL5SMPMt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Transformation\n",
        "\n",
        "Linear transformations are vector-valued functions that take D-dimensional  vectors $x \\in \\mathbb{R}^{D}$ and transform them into M-dimensional vectors, $y \\in \\mathbb{R}^{M}$. We are going to use linear transformation with [\"bias trick\"](https://en.wikipedia.org/wiki/Affine_transformation#Augmented_matrix) to implement the transformation:\n",
        "\n",
        "$$\n",
        "y = xW + b\n",
        "$$\n",
        "\n",
        "\n",
        "In this assignment, you will use this transformation in the SVM, softmax, and neural network classifiers.\n",
        "\n",
        "You will need to implement both the forward and backward direction of this linear layer. Take a look here for more details on the [backpropagation of a linear layer.](https://web.eecs.umich.edu/~justincj/teaching/eecs442/notes/linear-backprop.html)"
      ],
      "metadata": {
        "id": "VhFZKNmUXzt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** linear_forward(X, W) that returns linear transformations on data X using the augmented parameter matrix, W. "
      ],
      "metadata": {
        "id": "KszunLwwRJqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(x, w):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a linear transformation.\n",
        "\n",
        "    The input x has shape (N, D) and contains a minibatch of N\n",
        "    samples, where each sample x[i] has shape (D). We will \n",
        "    transform it to an output vector of dimension M.\n",
        "\n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, D)\n",
        "    - w: A numpy array of weights, of shape (D+1, M)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, M)\n",
        "    - cache: (x, w)\n",
        "\n",
        "    The returned (x, w) is redundant, but makes the constructing the entire layer\n",
        "    a little more concise.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    out = None # Initialize the out variable.\n",
        "\n",
        "    #\n",
        "    # PUT YOUR CODE BELOW: Below, implement the linear forward pass. Store the result in out.\n",
        "    # Make sure to do the bias trick! \n",
        "    x_prime = np.copy(x)\n",
        "    w_prime = np.copy(w)\n",
        "    N,D = np.shape(x_prime)\n",
        "    x_prime = np.append(x_prime, np.ones((N,1)), axis = 1)\n",
        "    x_prime[N-1,D] = 1\n",
        "    out = x_prime.dot(w)\n",
        "\n",
        "\n",
        "    # The lines below do not need to be changed.\n",
        "    cache = (x, w)\n",
        "    \n",
        "    return out, cache"
      ],
      "metadata": {
        "id": "fsl6QHm_PJdi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** linear_backward(dout, cache) that returns the analytical gradients with respect to X and W."
      ],
      "metadata": {
        "id": "VLnk9xwpRf2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_backward(d_upstream, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for an linear layer.\n",
        "\n",
        "    Inputs:\n",
        "    - d_upstream: Upstream derivative, of shape (N, M)\n",
        "    - cache: Tuple of:\n",
        "      - x: Input data, of shape (N, D)\n",
        "      - w: Weights, of shape (D+1, M)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient of the output of this layer with respect to x, of shape (N, D).\n",
        "          This is the downstream gradient.\n",
        "    - dw: Gradient with respect to w, of shape (D+1, M)\n",
        "    \"\"\"\n",
        "    x, w = cache\n",
        "    dx, dw = None, None\n",
        "\n",
        "    # PUT YOUR CODE BELOW: Implement the linear backward pass by calculating the\n",
        "    # gradient with respect to the cached inputs x and w. Store them in the \n",
        "    # variables dx and dw.\n",
        "    N, D = np.shape(x)\n",
        "    M = np.shape(w)[1]\n",
        "    dx = d_upstream.dot(w[:D,:].transpose())\n",
        "    bias = [np.sum(d_upstream, axis = 0)]\n",
        "    dw = np.append(x.transpose().dot(d_upstream),bias,axis = 0)\n",
        "\n",
        "\n",
        "   \n",
        " \n",
        "    # The lines below do not need to be changed.\n",
        "\n",
        "    return dx, dw"
      ],
      "metadata": {
        "id": "41ac6dd5TSEX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finite difference is the discrete analog of derivatives, used to implement gradients numerically. While analytical derivates are faster to compute, they tend to be difficult to implement and error prone. It is standard practice to perform a gradient check by comparing the analytical gradient implementation with a numerical gradient. \n",
        "\n",
        "The multi-variate central difference for a function $f(x,y)$ is given by:\n",
        "$$\n",
        "\\frac{\\partial  f}{\\partial x} = \\frac{f(x-h, y)-f(x+h, y)}{2h}\n",
        "$$\n",
        "and \n",
        "$$\n",
        "\\frac{\\partial  f}{\\partial y} = \\frac{f(x, y-h)-f(x, y+h)}{2h}\n",
        "$$\n",
        "\n",
        "The pattern holds for functions with higher number of variables. For central finite difference, generally $h=10^{-6}$. \n",
        "\n",
        "The multi-variate chain can be written as: \n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x_i} = \\sum^{m}_{\\mathcal{l}=1} \\frac{\\partial L}{\\partial y_l}\\frac{\\partial y_l}{\\partial x_i}\n",
        "$$\n",
        "\n",
        "This simplifies nicely as the gradient for each variable in the matrix the sum of the products of the upstream gradient. `d_upstream` and the finite difference matrix. \n"
      ],
      "metadata": {
        "id": "lk1XRA9LSKml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement**  the `finite_difference_linear` function the next cell to test your implementation of the linear_forward and linear_backward functions. "
      ],
      "metadata": {
        "id": "Je8LC4OnT-rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def finite_difference_linear(d_upstream, cache):\n",
        "    '''\n",
        "    Computes the numerical gradient for a linear layer\n",
        "\n",
        "    Inputs:\n",
        "    - d_upstream: Upstream derivative, of shape (N, M)\n",
        "    - cache: Tuple of:\n",
        "      - x: Input data, of shape (N, D)\n",
        "      - w: Weights, of shape (D+1, M)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, D).  This is the downstream\n",
        "          gradient.\n",
        "    - dw: Gradient with respect to w, of shape (D+1, M)\n",
        "    '''\n",
        "\n",
        "\n",
        "    # PUT YOUR CODE BELOW: Implement the finite difference for the linear\n",
        "    # function.  Return the gradient at input (x,w) w.r.t to x and w.\n",
        "    x,w = cache\n",
        "    N,D = np.shape(x)\n",
        "    M = np.shape(w)[1]\n",
        "    dx = np.zeros((N,D))\n",
        "    dw = np.zeros((D+1, M))\n",
        "    for row in range(N):\n",
        "      for column in range(D):\n",
        "        h = 10**(-6) \n",
        "        h_prime = np.zeros((N,D))\n",
        "        h_prime[row,column] = 10**(-6)\n",
        "        x_positive = x + h_prime\n",
        "        x_negative = x - h_prime\n",
        "        df_numerator = linear_forward(x_positive,w)[0] - linear_forward(x_negative,w)[0]\n",
        "        df = df_numerator/(2 * h)\n",
        "        for i in range(N):\n",
        "          for j in range(M):\n",
        "            dx[row,column] += df[i,j] * d_upstream[i,j]\n",
        "    for row in range(D+1):\n",
        "      for column in range(M):\n",
        "        h = 10**(-6) \n",
        "        h_prime = np.zeros((D+1,M))\n",
        "        h_prime[row,column] = 10**(-6)\n",
        "        w_positive = w + h_prime\n",
        "        w_negative = w - h_prime \n",
        "        df_numerator = linear_forward(x, w_positive)[0] - linear_forward(x,w_negative)[0]\n",
        "        df = df_numerator/(2 * h)\n",
        "        for i in range(N):\n",
        "          for j in range(M):\n",
        "            dw[row,column] += df[i,j] * d_upstream[i,j]\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    # The lines below do not need to be changed.\n",
        "    return dx, dw"
      ],
      "metadata": {
        "id": "HvUDtyAWUx7u"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run** this cell to do a gradient check to test the analytical gradients from the `linear_backward` function with `finite_difference_linear`.  "
      ],
      "metadata": {
        "id": "iTNHRnDDvCMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_check_linear():\n",
        "    N = 16\n",
        "    D = 4\n",
        "    C = 3\n",
        "\n",
        "    test_weight = np.random.random((D+1, C))\n",
        "    test_input = np.random.random((N, D))\n",
        "    dout = np.random.random((N, C))\n",
        "\n",
        "    cache = (test_input, test_weight)\n",
        "    \n",
        "    grad_x_numerical, grad_w_numerical = finite_difference_linear(dout, cache)\n",
        "    grad_x_analytical, grad_w_analytical = linear_backward(dout, cache)\n",
        "    check_input_gradient = np.allclose(grad_x_numerical, grad_x_analytical)\n",
        "    check_weight_gradient = np.allclose(grad_w_numerical, grad_w_analytical)\n",
        "\n",
        "    if not check_input_gradient:\n",
        "        print(\"The gradient with respect to x failed\")\n",
        "\n",
        "    if not check_weight_gradient:\n",
        "        print(\"The gradient respect to w failed\")\n",
        "    \n",
        "    print(\"gradient check for linear passed!\")\n",
        "\n",
        "gradient_check_linear()"
      ],
      "metadata": {
        "id": "jvmbfkfRPSZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e570eb-2481-4c2b-9b7f-b18b4f18d74e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gradient check for linear passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Linear Classifiers\n"
      ],
      "metadata": {
        "id": "i1DiweHpU-_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNIST Dataset\n",
        "\n",
        "MNIST is a dataset images of handwritten digits compiled by the National Institute of Standards and Technology (NIST). The dataset is widely used as a testing ground for machine learning algorithms for image classification.  \n",
        "\n",
        "The images are 28x28 pixels with only a single grayscale channel. You will be using 20,000 samples from the original training dataset for our next set of experiments. \n",
        "\n",
        "You will perform tests on the 10,000 sample test set.\n",
        "\n",
        "In this section, we will implement the linear classifiers with the MNIST dataset"
      ],
      "metadata": {
        "id": "38HrO_eBXjrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run** the following cell to define some helper functions to load the MNIST data."
      ],
      "metadata": {
        "id": "dU3af5u3vY2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist_data_parser_helper(csv_file_name):\n",
        "    X = []\n",
        "    Y = []\n",
        "    with open(csv_file_name,'r') as _file:\n",
        "        csv_reader = csv.reader(_file, delimiter=\",\")\n",
        "        for row in csv_reader:\n",
        "            Y.append(float(row[0]))\n",
        "            X.append([float(i) for i in row[1:]])\n",
        "    return (np.array(X), np.array(Y))\n",
        "\n",
        "def get_mnist_train_data():\n",
        "    X_train, Y_train = mnist_data_parser_helper(\"sample_data/mnist_train_small.csv\")\n",
        "    return X_train, Y_train\n",
        "\n",
        "def get_mnist_test_data():\n",
        "    X_test, Y_test = mnist_data_parser_helper(\"sample_data/mnist_test.csv\")\n",
        "    return X_test, Y_test"
      ],
      "metadata": {
        "id": "y6p9vHGhfBzg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run** the following cell to visualize some of the samples from the MNIST dataset."
      ],
      "metadata": {
        "id": "nsEncnA3vh9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = get_mnist_train_data()\n",
        "\n",
        "# Visualize some examples from the dataset.\n",
        "# We show a few examples of training images from each class.\n",
        "classes = list(range(10))\n",
        "\n",
        "num_classes = len(classes)\n",
        "samples_per_class = 7\n",
        "for y, cls in enumerate(classes):\n",
        "    idxs = np.flatnonzero(y_train.astype('uint8') == y)\n",
        "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "    for i, idx in enumerate(idxs):\n",
        "        plt_idx = i * num_classes + y + 1\n",
        "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
        "        plt.imshow(x_train[idx].astype('uint8').reshape(28,28), cmap='gray')\n",
        "        plt.axis('off')\n",
        "        if i == 0:\n",
        "            plt.title(cls)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "21II-zCpe-ER",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "6006ab10-29ce-40ed-a09c-95b9d405a929"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 70 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run** the following cell to gather the training dataset.\n"
      ],
      "metadata": {
        "id": "SLbwp3V-vxg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = get_mnist_train_data()  # Get the training dataset"
      ],
      "metadata": {
        "id": "ZY8RrHx4Fegk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run** the following gather the testing dataset. "
      ],
      "metadata": {
        "id": "RHijm2aiv0cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, y_test = get_mnist_test_data()     # Get the test dataset"
      ],
      "metadata": {
        "id": "84cjF778Ag9O"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run** the following normalize the input dataset to have a mean of 0 and standard deviation of 1. "
      ],
      "metadata": {
        "id": "d4cTK2RDv5wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_mean = x_train.mean()\n",
        "x_std = x_train.std()\n",
        "\n",
        "x_train = (x_train - x_mean)/(x_std)\n",
        "x_test = (x_test - x_mean)/(x_std)"
      ],
      "metadata": {
        "id": "n7RKsVNmfTx7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run** the following cell to check the dimensions of the data."
      ],
      "metadata": {
        "id": "LfRlRMS9PTZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, dim = x_train.shape\n",
        "N_test, _ = x_test.shape\n",
        "print(f\"Number of training sample {N} with {dim} pixels per image\")\n",
        "print(f\"Number of training sample {N_test} with {dim} pixels per image\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59jB-sjwQNVL",
        "outputId": "aaa0ab44-2975-434e-e766-6eb69bed1012"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sample 20000 with 784 pixels per image\n",
            "Number of training sample 10000 with 784 pixels per image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement and Evaluate ML Models\n",
        "\n",
        "In this problem we are optimizing parametric ML models on high-dimensional real world data. As we've seen before in our optimization tasks, even simple polynomial functions can have many local minima that our optimizers can get stuck on.\n",
        "\n",
        "\n",
        "Therefore, we need to be able to evaluate how well our model was optimized on the data, especially on the with different hyperparameters. Hyperparameters are parameters that control the learning process that are not updated during the training process, such learning rate or model size.\n",
        "\n",
        "\n",
        "For large enough datasets, a common strategy is to use [validation sets](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets) to evaluate the [best model](https://en.wikipedia.org/wiki/Model_selection). In the next cell, split the training dataset into training and validation set with an 80-20 split ratio. "
      ],
      "metadata": {
        "id": "TDV7xaD_BkEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** the following cell to split the `x_train` and `y_train` arrays to training and validations sets with an 80-20 split ratio. Place the split arrays in to the `DATA` dictionary. This dictionary will be used to feed data into the `Solver`."
      ],
      "metadata": {
        "id": "wi9rQwaUWBHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform training and validation dataset splits\n",
        "\n",
        "# PUT YOUR CODE BELOW\n",
        "DATA = {\"X_train\": x_train[:16000],      # Replace with the value here\n",
        "        \"X_val\" : x_train[16000:],       # Replace with the value here\n",
        "        \"Y_train\" : y_train[:16000],     # Replace with the value here\n",
        "        \"Y_val\" : y_train[16000:]}       # Replace with the value here\n"
      ],
      "metadata": {
        "id": "e0gRbRkAEO5C"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimizing Algorithm\n",
        "\n",
        "**Run** the following cell to define the stochastic gradient descent algorithm that we will use to optimize our models. "
      ],
      "metadata": {
        "id": "xXdYUcfTa78v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(w, dw, lr=1e-2):\n",
        "    \"\"\"\n",
        "    Performs vanilla stochastic gradient descent.\n",
        "\n",
        "    config format:\n",
        "    - learning_rate: Scalar learning rate.\n",
        "    \"\"\"\n",
        "\n",
        "    w -= lr * dw\n",
        "    return w"
      ],
      "metadata": {
        "id": "elsI_qFOa8HB"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Solver\n",
        "\n",
        "\n",
        "The following solver class optimizes a given model using mini-batch gradient optimizations. \n",
        "\n",
        "**Run** the following cell to define the `Solver` class. "
      ],
      "metadata": {
        "id": "haKibBipbrmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Solver(object):\n",
        "    \"\"\"\n",
        "    Solver class for the learnable models using \n",
        "    mini-batch gradient descent.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 data,\n",
        "                 learning_rate=1e-3,\n",
        "                 num_epochs=50,\n",
        "                 batch_size=200,\n",
        "                 validation_frequency=16):\n",
        "        \"\"\"\n",
        "        Construct a new Solver instance.\n",
        "\n",
        "        Inputs:\n",
        "          model: Python class equiped with forward, backward, predict methods and a params dictionary\n",
        "          data: Dictionary with X_train, X_val,  Y_train, Y_val keys\n",
        "          learning_rate: Float, step size of the optimizer\n",
        "          num_epochs: Int, Number of times to completely traverse X_train\n",
        "          batch_size: Int, The number of samples in update\n",
        "          validation_frequency: Int, Solver performs validation loop every validation_frequency batches.\n",
        "                               Set this to a high number if num_epochs is large\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        \n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.validation_frequency = validation_frequency\n",
        "\n",
        "        self.num_training = data[\"X_train\"].shape[0]\n",
        "        self.input_dim = data[\"X_train\"].shape[1]\n",
        "        \n",
        "        intervals = list(range(0, self.num_training, batch_size))[1:]\n",
        "        \n",
        "        self.X_train = np.array_split(data[\"X_train\"], intervals, axis=0)\n",
        "        self.y_train = np.array_split(data[\"Y_train\"], intervals)\n",
        "\n",
        "        self.num_batches_in_training = len(self.X_train)\n",
        "\n",
        "        self.X_val = data[\"X_val\"]\n",
        "        self.y_val = data[\"Y_val\"]\n",
        "\n",
        "        self.update_rule = sgd\n",
        "        self.loss_history = []\n",
        "        self.validation_history = []\n",
        "\n",
        "        self.iteration_num = 0\n",
        "\n",
        "\n",
        "    def _step(self, batch_id):\n",
        "        \"\"\"\n",
        "        Make a single gradient update. This is called by train() and should not\n",
        "        be called manually.\n",
        "        \"\"\"\n",
        "        # Make a minibatch of training data\n",
        "        X_batch = self.X_train[batch_id]\n",
        "        y_batch = self.y_train[batch_id]\n",
        "\n",
        "        # Compute loss and gradient\n",
        "        score, cache = self.model.forward(X_batch)\n",
        "        loss, dL = self.model.loss(score, y_batch)\n",
        "        _, grads = self.model.backward(dL, cache)\n",
        "\n",
        "        self.loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in self.model.params.items():\n",
        "            dw = grads[p]\n",
        "            next_w = self.update_rule(w, dw, self.learning_rate)\n",
        "            self.model.params[p] = next_w\n",
        "    \n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Optimization to train the model\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for batch_id in range(self.num_batches_in_training):\n",
        "                \n",
        "                self._step(batch_id)\n",
        "\n",
        "                self.iteration_num += 1\n",
        "\n",
        "                if (self.iteration_num % self.validation_frequency == 0):\n",
        "                    self.validate()\n",
        "\n",
        "        self.validate()\n",
        "\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"\n",
        "        Checks the validation error of the model at the time it is being called.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        N = self.y_val.shape[0]\n",
        "        predictions = self.model.predict(self.X_val)\n",
        " \n",
        "        accuracy = np.count_nonzero(predictions == self.y_val.astype(int))\n",
        "\n",
        "        print(f\"The validation accuracy at iteration {self.iteration_num}  is \\\n",
        "              {(float(accuracy)/N)*100}%\")"
      ],
      "metadata": {
        "id": "kQP6qra2bpOL"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a) Linear Classifier\n",
        "\n",
        "We will be using two linear classifiers on the (MNIST) dataset. Using the linear transform you’ve implemented before, you will use SGD to train a multiclass support vector machine (SVM) and softmax classifiers to predict the handwritten digits.\n",
        "\n",
        "The linear classifier base class implements training and prediction methods shared by the linear classifiers. \n",
        "\n",
        "**Implement** the following cell to complete the definition of the following `LinearClassifier` class."
      ],
      "metadata": {
        "id": "vZfssCcKavEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the `__init__`, `forward`, `backward`, and `predict` methods. \n",
        "\n",
        "Complete the following:\n",
        "- The `__init__()` method initializes the class. You must generate a random weight matrix of shape `(input_dim+1,num_classes)`  \n",
        "- The `forward()` method generates the scores for given an input sample, by applying a `linear_forward()` transformation on the inputs `x` and weights matrix `self.params['W1']`\n",
        "- The `backward()` method returns the gradients with respect to the inputs and weights, using the `linear_backward()` method. Make sure the key for the returned dictionary `weights_gradient` matches the paramts dictionary.\n",
        "- The `predict()` method returns the labels predicted from the scores returned using the `self.forward()` method. "
      ],
      "metadata": {
        "id": "VbsDBRjmYeUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearClassifier(object):\n",
        "    \"\"\"\n",
        "    The base class for the linear classifier. \n",
        "\n",
        "    Note that this class does not implement gradient descent; instead, it\n",
        "    will interact with a separate Solver object that is responsible for running\n",
        "    optimization.\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim=784,\n",
        "                 num_classes=10):\n",
        "        self.params = {}\n",
        "        self.input_features = input_dim\n",
        "        self.num_classes = 10\n",
        "\n",
        "        # PUT YOUR CODE BELOW:                                                    \n",
        "        # Initialize the weights of the linear classifier. Weights should be      \n",
        "        # initialized from a Gaussian centered at 0.0 with standard deviation     \n",
        "        # equal to 1e-3, and biases should be initialized to zero.                   \n",
        "        # Store in the self.W dictionary with key name 'W1'                       \n",
        "                                                                                 \n",
        "        # The lines below do not need to be changed in the method\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - x: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "\n",
        "        Outputs:\n",
        "        A list containing the value of the loss function at each training iteration.\n",
        "        \"\"\"\n",
        "        num_train, dim = x.shape\n",
        "        num_classes = self.num_classes\n",
        "        out = None\n",
        "        cache = None\n",
        "        \n",
        "        # PUT YOUR CODE BELOW:                                                    \n",
        "        # Implement this method. Generate the scores in out and store the old      \n",
        "        # values into the cache.                                                  \n",
        "            \n",
        "        # The lines below do not need to be changed\n",
        "         \n",
        "    \n",
        "        return out, (cache, )\n",
        "\n",
        "    def backward(self, dout, cache):\n",
        "        weight_gradients = {}\n",
        "        dx = None\n",
        "        \n",
        "        # PUT YOUR CODE BELOW:                                                                   \n",
        "        # Implement this method. Generate the gradients with respect to x from \n",
        "        # cache and set it dx, the upstream error signal.\n",
        "        # Store the gradient with respect to the weights in the weights_gradients\n",
        "        # dictionary. Make sure the key matches the ket of the params dictionary    \n",
        "\n",
        "        # The lines below do not need to be changed \n",
        "\n",
        "        return (dx, weight_gradients)\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this linear classifier to predict labels for\n",
        "        data points.\n",
        "\n",
        "        Inputs:\n",
        "        - x: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "          array of length N, and each element is an integer giving the predicted\n",
        "          class.\n",
        "        \"\"\"\n",
        "        y_pred = np.zeros(x.shape[0])\n",
        "        \n",
        "        # PUT YOUR CODE BELOW:                                                                   \n",
        "        # Implement this method. Store the predicted labels in y_pred.            \n",
        "\n",
        "\n",
        "        # The lines below do not need to be changed\n",
        "\n",
        "        return y_pred\n",
        "    \n",
        "\n",
        "    def loss(self, scores, y_batch):\n",
        "        \"\"\"\n",
        "        Compute the loss function and its derivative.\n",
        "        Subclasses will override this.\n",
        "\n",
        "        Inputs:\n",
        "        - scores: A numpy array of shape (N, C) containing a minibatch of N\n",
        "          data points; each point has dimension C, where C is the number of classes.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "        - reg: (float) regularization strength.\n",
        "\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to scores; an array of the same shape as scores\n",
        "        \"\"\"\n",
        "        # The lines below do not need to be changed\n",
        "        # Do not implement anything here. The subclasses will override this method\n",
        "        pass"
      ],
      "metadata": {
        "id": "oLgT_g66T71G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b) Support Vector Machine\n",
        "\n",
        "\n",
        "The LinearSVM class defines an SVM-based linear classifier. The classifier uses the hinge loss to optimize the model parameters. \n",
        "\n",
        "The Hinge loss for an input sample $x_i$ (a vector) is given by: \n",
        "\n",
        "$$\n",
        "L_i = \\sum_{j \\neq y_i} \\text{max}(0, s_j-s_{y_i}+1)\n",
        "$$\n",
        "\n",
        "\n",
        "Where, $y_i$ is the label of the $i$-th sample.  The label is the correct class label where $0 \\leq y_i \\lt C$, where C is the number of classes.   The scalar $s_{y_i}$ is the $y_i$-th element of the score vector. \n",
        "\n",
        "The average loss over N samples is therefore:\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{N}\\sum^{N}_{i=1}L_i\n",
        "$$\n",
        "\n",
        "The per-sample gradient of the loss w.r.t. the score $s_j$ is given by:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial  L_i}{\\partial s_j} = \\left \\{\n",
        "\\begin{array}{ll}\n",
        "0 & j = s_{y_i} \\text{ or } s_j-s_{y_i}+1 \\leq 0    \\\\\n",
        "1 & j \\neq s_{y_i} \\text{ and } s_j-s_{y_i}+1 > 0 \\\\\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "**Implement** the `svm_loss` function in the following cell. Store the average loss the `loss` variable and the gradient w.r.t `scores` in the `dy` variable. This is the loss over multiple samples, therefore you should take the mean of the loss. \n"
      ],
      "metadata": {
        "id": "rS6qcaqUcyqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_loss(scores, y_batch):\n",
        "    \"\"\"\n",
        "    Returns hinge loss of the scores and y_batch. \n",
        "\n",
        "    Inputs:\n",
        "    - scores: A numpy array of shape (N, C) containing a minibatch of N\n",
        "      data points; each point has dimension C, where C is the number of classes.\n",
        "    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "    - reg: (float) regularization strength.\n",
        "\n",
        "    Returns: A tuple containing:\n",
        "    - loss as a single float\n",
        "    - gradient with respect to scores; an array of the same shape as scores\n",
        "    \"\"\"\n",
        "    loss = 0\n",
        "    dy = np.zeros(scores.shape)  \n",
        "    # PUT YOUR CODE BELOW:                                                       \n",
        "    # Implement the structured SVM loss, storing the  \n",
        "    # result in loss. Make sure to take the mean of the loss.\n",
        "    # Hint: The intermediate results maybe useful for the gradient calculation                                                        \n",
        "    \n",
        "  \n",
        "    # PUT YOUR CODE BELOW:                                                                    \n",
        "    # Implement the gradient for the SVM loss, storing the result    \n",
        "    # in dy.                                                                    \n",
        "    #                                                                           \n",
        "    # Hint: Instead of computing the gradient from scratch, it may be easier    \n",
        "    # to reuse some of the intermediate values that you used to compute the     \n",
        "    # loss.                                                                     \n",
        "\n",
        "\n",
        "    # The lines below do not need to be changed.\n",
        "    return loss, dy"
      ],
      "metadata": {
        "id": "S-_mC08N89UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run** the following cell to define the `LinearSVM` class with your implementation of the `svm_loss`"
      ],
      "metadata": {
        "id": "MMaObHBhamT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearSVM(LinearClassifier):\n",
        "    \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
        "\n",
        "    def loss(self, scores, y_batch):\n",
        "        return svm_loss(scores, y_batch)"
      ],
      "metadata": {
        "id": "VD43ltPTcw-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SVM Experiments\n",
        "\n",
        "In the next few cells run the `Solver` with SVM models on the training and validation data you've defined previously. Use the `DATA` dictionary you defined previously as the data parameter.\n",
        "\n",
        "As you have seen with previous assignments, optimizations can be highly dependent on the hyperparameters of the model. You should try multiple models with different learning rates. You may also increase the amount of time you train by increasing the number of epochs. \n",
        "\n",
        "Keep the top-5 best performing models and the worst performing model on the validation set.\n"
      ],
      "metadata": {
        "id": "DNPZ7RoU2Oyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** hyperparameter validation loop in the next cell to train multiple models with different hyperparameters. Keep the top-5 best performing models on the validation set. You can try different learning rates. You may change the num_epochs, but be wary of timeouts. "
      ],
      "metadata": {
        "id": "DD7et1xEF9h6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "US364cOjGC5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** the testing performance of your top-5 performing models on the test set and print the results. "
      ],
      "metadata": {
        "id": "Dl4R1tyz4VpS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "71qBuIr75ArL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** the next cell to visualize the weights corresponding to each sample in the *best* performing SVM models. You should have ten 28x28 images.\n",
        "\n",
        "Make sure to rescale the  weights to be between 0 and 255.\n",
        "\n",
        "Depending on your learning rate and weights, it might not look so great. If all the images look the same but you have good accuracy, try subtracting the average of weights from weights of each class. \n",
        "\n",
        "You can add additional cells below."
      ],
      "metadata": {
        "id": "srNp8XPEbVAt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hVDxnLZCbVQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (c) Cross-Entropy Loss\n",
        "\n",
        "The Softmax class defines the cross-entropy loss for training and prediction methods like the previous the linear classifiers.\n",
        "The cross-entropy loss is actually the composition of two distinct functions: the softmax function and the cross-entropy.\n",
        "However,\n",
        "we commonly refer to it just as the cross-entropy loss,\n",
        "with the implicit understanding that for deep learning,\n",
        "the cross-entropy is not computed on the raw scores,\n",
        "but rather the softmax of the raw scores.\n",
        "\n",
        "For a score vector $s$, the softmax activation of the $j$-th element is given by,\n",
        "\n",
        "$$\n",
        "\\sigma_j = \\frac{e^{s_{j}}}{\\sum^{C - 1}_{k=0}e^{s_k}}\n",
        "$$\n",
        "\n",
        "A simple implementation of the softmax function can be numerically unstable. Large scores can result in an overflow. Large scores are normalized to be not too big or too small. [Check here for more details.](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/#:~:text=Computing%20softmax%20and%20numerical%20stability)\n",
        "\n",
        "\n",
        "The cross-entropy is a measure of the difference between two probability distributions.\n",
        "In the general case,\n",
        "the cross-entropy $H$ between the true probability distribution $P$ and the estimated probability distribution $Q$ is given by:\n",
        "\n",
        "$$\n",
        "H(P, Q)=-\\sum_{x \\in \\mathcal{X}} P(x) \\log Q(x)\n",
        "$$\n",
        "\n",
        "where $\\mathcal{X}$ is the event space.\n",
        "It is a measure of how \"far off\" our estimated distribution $Q$ is from $P$.\n",
        "(Note that because $P$ and $Q$ are actually functions,\n",
        "$H$ in this case is a function operating on functions, also known as an *operator*.)\n",
        "\n",
        "In our case,\n",
        "$P$ is zero except for the correct label,\n",
        "and thus the cross-entropy reduces to simply the negative logarithm of the score corresponding to the correct class,\n",
        "which is just\n",
        "\n",
        "$$\n",
        "L_i = -\\log(\\sigma_{y_i}))\n",
        "$$\n",
        "\n",
        "where $y_i$ is the correct label of the $x_i$ input sample,\n",
        "and $\\sigma_{y_i}$ is the softmax output of the corresponding correct label. $L$ is then just the average over the $L_i$.\n",
        "\n",
        "The derivative of the softmax is given by, \n",
        "\n",
        "$$\n",
        "\\frac{\\partial\\sigma_i}{\\partial s_j} = \\left \\{\n",
        "\\begin{array}{ll}\n",
        "\\sigma_i(1 - \\sigma_{j}) & i = j     \\\\\n",
        "-\\sigma_i\\sigma_j & i \\neq j  \\\\\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "Details on the derivation can be found [here](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/).\n",
        "\n",
        "The derivative of the negative logarithm is given by, \n",
        "\n",
        "$$\n",
        "\\frac{\\partial (-\\log)}{\\partial \\sigma_{y_i}} = -\\frac{1}{\\sigma_{y_i}}\n",
        "$$"
      ],
      "metadata": {
        "id": "nhA92akOczjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** the `cross_entropy_loss` function in the following cell. The function returns a tuple of `(loss, dy)` where, `loss` is the cross-entropy loss based on the inputs, and `dy` is the gradient of the loss with respect to the `scores` input.This is the loss over multiple samples, therefor you should take the mean of the loss. "
      ],
      "metadata": {
        "id": "XJDqbXH4yxZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(scores, y_batch):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy layer\n",
        "\n",
        "    Inputs:\n",
        "    - scores: A numpy array containing the scores, of shape (N, C)\n",
        "    - y_batch: A numpy array containing the labels, of shape (N, 1)\n",
        "\n",
        "    Returns: A tuple containing:\n",
        "    - loss as a single float\n",
        "    - gradient with respect to scores; an array of the same shape as scores\n",
        "    \"\"\"\n",
        "\n",
        "    loss = 0\n",
        "    dy = np.zeros(scores.shape)\n",
        "\n",
        "    # PUT YOUR CODE BELOW:                                                       \n",
        "    # Implement the cross-entropy loss, storing the  \n",
        "    # result in loss. Make sure to take the mean of the loss.\n",
        "    # Hint: The intermediate results maybe useful for the gradient calculation  \n",
        "\n",
        "\n",
        "\n",
        "    # PUT YOUR CODE BELOW:                                                                    \n",
        "    # Implement the gradient for the cross-entropy loss, storing the result    \n",
        "    # in dy.                                                                    \n",
        "    #                                                                          \n",
        "    # Hint: Instead of computing the gradient from scratch, it may be easier    \n",
        "    # to reuse some of the intermediate values that you used to compute the     \n",
        "    # loss. \n",
        "\n",
        "    # The lines below do not need to be changed.\n",
        "    return loss, dy"
      ],
      "metadata": {
        "id": "5hIEv5fl9llO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run** the following cell to define the `Softmax` classifier class."
      ],
      "metadata": {
        "id": "8h0EQF-yuu8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax(LinearClassifier):\n",
        "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
        "\n",
        "    def loss(self, scores, y_batch):\n",
        "        return cross_entropy_loss(scores, y_batch)"
      ],
      "metadata": {
        "id": "RKYgCIcHczy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Softmax Experiments\n",
        "\n",
        "In the next few cells run the `Solver` with Softmax models on the training and validation data you've defined previously, similar to the SVM experiements.  Use the `DATA` dictionary you defined previously as the data parameter.\n",
        "\n",
        "\n",
        "Keep the top-5 best performing models and the worst performing model on the validation set."
      ],
      "metadata": {
        "id": "Ug02WvIg5DT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** hyperparameter validation loop in the next cell to train multiple models with different hyperparameters. Keep the top-5 best performing models on the validation set. You can try different learning rates. You may change the num_epochs, but be wary of timeouts. "
      ],
      "metadata": {
        "id": "qRFvUnysFwvt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05Od5fTg5Dqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** the next cell to visualize the weights corresponding to each sample in the *best* performing Softmax models. You should have ten 28x28 images.\n",
        "\n",
        "\n",
        "You can add additional cells below.\n",
        "\n",
        "Depending on your learning rate and weights, it might not look so great. If all the images look the same but you have good accuracy, try subtracting the average of weights from weights of each class. "
      ],
      "metadata": {
        "id": "s8j9KoxfXgY5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0SEDP9m-EZNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** the testing performance of your top-5 performing Softmax models on the test set and print the results. "
      ],
      "metadata": {
        "id": "4GdpbgYg5D8W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oxuJHeU45EVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Deeper Neural Networks (Very Slightly)\n",
        " "
      ],
      "metadata": {
        "id": "A7mX-suZStG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Up until now, we have been working with linear classification models. Linear classification models are very adept at modelling data that have nice linear boundaries. In practice, realy world data is rarely linear. Multilayer fully-connected neural networks with non-linear activation functions on the other hand can model non-linear data-label relationships. \n",
        "\n",
        "Such models are a powerful extension to linear models and are the building blocks of modern deep learning. \n",
        "\n",
        "In this section, you will be implementing a two-layer fully-connected neural network. Fully-connected neural networks perform the transformation that you've implemented above coupled with a non-linear activation function. You will also implement your own version of the rectified linear unit fuction (commonly reffered to as ReLU), a non-linear activation function."
      ],
      "metadata": {
        "id": "j60wKxwhXu2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (a) ReLU Function\n",
        "\n",
        "The ReLU function is given by:\n",
        "\n",
        "$$\n",
        "f(x) = x^{+} = max(0, x)\n",
        "$$\n",
        "\n",
        "**Implement** the following cell to complete the definition of the `ReLU_forward` function."
      ],
      "metadata": {
        "id": "gP_YgE1JWj1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a ReLU actiivation.\n",
        "\n",
        "    The input x has shape (N, D) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (D). We will \n",
        "    transform it to an output vector of dimension M.\n",
        "\n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, D)\n",
        "\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, D)\n",
        "    - cache: (x)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    \n",
        "    # PUT YOUR CODE BELOW: Implement the ReLU forward pass. Store the result in \n",
        "    # out. You will need to reshape the input into rows.\n",
        "\n",
        "    \n",
        "    # The lines below do not need to be changed.\n",
        "\n",
        "    cache = (x,)\n",
        "    return out, cache"
      ],
      "metadata": {
        "id": "tQ-CcTqJStgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ReLU derivative is given by: \n",
        "\n",
        "$$\n",
        "\\frac{\\partial f(x)}{\\partial x} = \\left\\{\n",
        "\\begin{array}{ll}\n",
        "      0 & x \\leq 0 \\\\\n",
        "      1 & x > 0 \\\\\n",
        "\\end{array}\n",
        "\\right. \n",
        "$$\n",
        "\n",
        "Where, $f(x)$ is the ReLU function of course.\n",
        "\n",
        "\n",
        "**Implement** the following cell to complete the definition of the `ReLU_backward` function."
      ],
      "metadata": {
        "id": "g3LU7V4Ib4xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU_backward(d_upstream, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for an linear layer.\n",
        "\n",
        "    Inputs:\n",
        "    - d_upstream: Upstream derivative, of shape (N, D)\n",
        "    - cache: Tuple of:\n",
        "      - x: Input data, of shape (N, D)\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, D)\n",
        "    \"\"\"\n",
        "    x,  = cache\n",
        "    dx = None\n",
        "\n",
        "    # PUT YOUR CODE BELOW: Implement the ReLU backward pass.                                 \n",
        "\n",
        "    # The lines below do not need to be changed.\n",
        "\n",
        "    return (dx, )"
      ],
      "metadata": {
        "id": "fTn6t0FaVBSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (b) Two-layer Neural Network\n",
        "\n",
        "**Implement** the definition of the two-layer neural network below.\n",
        "\n",
        "Similar to the `LinearClassifier` class, you should write the `__init__`, `forward`, `backward`, and `predict` methods. We will be using the cross-entropy loss for this network. \n",
        "\n",
        "Complete the following:\n",
        "- The `__init__()` method initializes the class. You must generate two random weight matrices. We will be using the bias trick, so the bias should concatenated to the weight matrix. They are initialized differently. \n",
        "\n",
        "- The `forward()` method generates the scores for given an input sample, by applying a `linear_forward()` and `ReLU_forward()` with appropriate inputs. Make sure to store and return the cache for the intermediate steps. \n",
        "\n",
        "- The `backward()` method returns the gradients with respect to the inputs and weights, using the `linear_backward()` and `ReLU_backward()`. Make sure the keys for the returned dictionary `weights_gradient` matches the keys in the `self.params` dictionary.\n",
        "\n",
        "- The `predict()` method returns the labels predicted from the scores returned using the `self.forward()` method.\n"
      ],
      "metadata": {
        "id": "jYDMTehxYP75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNet(object):\n",
        "    \"\"\"\n",
        "    A two-layer fully-connected neural network with ReLU nonlinearity and\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecure should be transform - relu - transform - softmax.\n",
        "\n",
        "    Note that this class does not implement gradient descent; instead, it\n",
        "    will interact with a separate Solver object that is responsible for running\n",
        "    optimization.\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 hidden_dim=100,\n",
        "                 num_classes=10,\n",
        "                 weight_scale=1e-3):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "\n",
        "        self.params = {}\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_dim = input_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # PUT YOUR CODE BELOW: Initialize the weights of the two-layer net. Weights should be     \n",
        "        # initialized from a Gaussian centered at 0.0 with standard deviation      \n",
        "        # equal to weight_scale, and biases should be initialized to zero.         \n",
        "        # All weights should be stored in the dictionary self.params, with first   \n",
        "        # layer weights and using the keys 'W1' and second layer weights and using \n",
        "        # the keys 'W2'. Make sure to concatenate the weights and biases to make a \n",
        "        # a single matrix for the bias trick!                                                            \n",
        "\n",
        "\n",
        "        # The lines below do not need to be changed in this method.\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Implement the forward pass of the neural network and return the scores\n",
        "\n",
        "        Inputs:\n",
        "        - x: A numpy array containing input data, of shape (N, self.input_dim)\n",
        "        \n",
        "\n",
        "        Returns a tuple of:\n",
        "        - out: output, of shape (N, D)\n",
        "        - Tuple of tuples:\n",
        "          - cache_lin_1: A tuple (x, w1) \n",
        "            - x: data, of shape (N, self.input_dim)\n",
        "            - w1: Weight of linear layer 1 of shape (self.input_dim+1, self.hidden_dim)\n",
        "          - cache_relu_1: A tuple (h, )\n",
        "            - h : data, of shape (N, self.hidden_dim)\n",
        "          - cache_lin_2:  A tuple (h, w2)\n",
        "            - h: data, of shape (N, self.hidden_dim)\n",
        "            - w2: weight of linear 2 of shape (self.hidden_dim+1, C)\n",
        "        \"\"\"\n",
        "        out = None\n",
        "        N, feature_dim = x.shape\n",
        "        cache_lin_1, cache_relu_1, cache_lin_2 = None, None, None\n",
        "\n",
        "        if (feature_dim != self.input_dim):\n",
        "            raise Exception(f\"The input feature dimension of {feature_dim} does \\\n",
        "                            not match the expected feature dimension of \\\n",
        "                            {self.input_dim} \")\n",
        "\n",
        "        \n",
        "        # PUT YOUR CODE BELOW: Perform a forward pass of the two-layer net.\n",
        "        # The architecture is transform - relu - transform \n",
        "        # Make to store the appropriate cache in the appropriate variables                     \n",
        "\n",
        "\n",
        "        # The lines below do not need to be changed in this method.\n",
        "\n",
        "        return out, (cache_lin_1, cache_relu_1, cache_lin_2)\n",
        "\n",
        "\n",
        "    def backward(self, dout, cache):\n",
        "        \"\"\"\n",
        "        Implement the backward pass of the neural network and return the\n",
        "        gradients w.r.t the input, and the weights\n",
        "\n",
        "        Inputs:\n",
        "        - dout: Upstream derivative, of shape (N, C)\n",
        "        - cache: Tuple of tuples:\n",
        "          - cache_lin_1: A tuple (x, w1) \n",
        "            - x: data, of shape (N, self.input_dim)\n",
        "            - w1: Weight of linear layer 1 of shape (self.input_dim+1, self.hidden_dim)\n",
        "          - cache_relu_1: A tuple (h, )\n",
        "            - h : data, of shape (N, self.hidden_dim)\n",
        "          - cache_lin_2:  A tuple (h, w2)\n",
        "            - h: data, of shape (N, self.hidden_dim)\n",
        "            - w2: weight of linear 2 of shape (self.hidden_dim+1, C)\n",
        "\n",
        "        Returns a tuple of:\n",
        "          - dx: A numpy array of the gradient with respect to x, of shape (N, D)\n",
        "          - weight_gradients: A dictionary of numpy arrays containing the\n",
        "              gradients with respect to the weights. \n",
        "        \"\"\"\n",
        "\n",
        "        weight_gradients = {}\n",
        "        dx = None\n",
        "\n",
        "        N, classes = dout.shape\n",
        "        \n",
        "        cache_lin_1, cache_relu_1, cache_lin_2 = cache\n",
        "\n",
        "        if (classes != self.num_classes):\n",
        "            raise Exception(f\"The output class dimension of {classes} does \\\n",
        "                            not match the expected number of classes \\\n",
        "                            {self.num_classes} \")\n",
        "\n",
        "        # PUT YOUR CODE BELOW: Perform a backward pass of the two-layer net.                      \n",
        "\n",
        "\n",
        "        # The lines below do not need to be changed in this method.\n",
        "        return (dx, weight_gradients)\n",
        "    \n",
        "    def predict(self, x):\n",
        "\n",
        "        \"\"\"\n",
        "        Implement the predictions from the forward pass of the neural network and \n",
        "        returns it. \n",
        "\n",
        "        Inputs:\n",
        "        - x: Input data, of shape (N, self.input_dim)\n",
        "        \n",
        "        Returns a tuple of:\n",
        "          - predictions: A numpy array of shape (N, ) of the predicted class per sample \n",
        "          \n",
        "        \"\"\"\n",
        "\n",
        "        y_pred = None\n",
        "\n",
        "        \n",
        "        # PUT YOUR CODE BELOW: Predict the classes of using the two-layer net.                    \n",
        "\n",
        "\n",
        "        # The lines below do not need to be changed in this method.\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def loss(self, scores, y_batch):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        # The lines below do not need to be changed in this method.\n",
        "        return cross_entropy_loss(scores, y_batch)"
      ],
      "metadata": {
        "id": "dSr_3dSiYQOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (c) Experiments\n",
        "\n",
        "Similar to the linear classifiers, you also want to identify the best configuration of hyperparameters that perform the best for your dataset. Similar to the case of the linear models, you can vary the learning rate for your solver. You should use the `Solver` class for these models as well. Use the `DATA` dictionary you defined previously as the data parameter.\n",
        "\n",
        "Additionaly, the neural network provides another hyperparameter to vary, the the number of neurons in the hidden layer. \n",
        "\n",
        "Adding a large of number of neurons may cause a large degradation in performance, the linear transformation scales as $O(N^3)$ with the number of neurons. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yovrIeNablvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** a hyperparameter validation loop in the next cell to train multiple models with different hyperparameters. Keep the top-5 best performing models on the validation set. You may change learning rate, and hidden dims. You may change the num_epochs, but be wary of timeouts. "
      ],
      "metadata": {
        "id": "N8zQgNiK-I9g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwZysCge-Js5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** the testing performance of your top-5 performing NN models on the test set and print the results. You can add additional cells below."
      ],
      "metadata": {
        "id": "zi1hTKxU-Kap"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QL6IbrNo-PTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** visualization for the `W1` weights of the *best* performing NN models. There are `hidden_dim` many of them per model. You should visualize a subset of the weights. You can select the columns at random. \n",
        "\n",
        "You can add additional cells below."
      ],
      "metadata": {
        "id": "qC8IL-RNdCOY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-CGHrOtdCb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}